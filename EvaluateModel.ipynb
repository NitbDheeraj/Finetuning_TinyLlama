{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2b7563-e6c6-4fe0-918e-600abee5d507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "import re\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2b06fa1-7298-4804-a1a6-0434d57c3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_finetuned_model(model_path, test_data_path, num_samples=100):\n",
    "    \"\"\"\n",
    "    Evaluate the fine-tuned model on test data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    print(\"Loading model and tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_path,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "    )\n",
    "    \n",
    "    # Load test data\n",
    "    print(\"Loading test data...\")\n",
    "    with open(test_data_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    # Sample test cases for evaluation\n",
    "    if len(test_data) > num_samples:\n",
    "        test_samples = test_data[:num_samples]\n",
    "    else:\n",
    "        test_samples = test_data\n",
    "    \n",
    "    results = []\n",
    "    exact_match_count = 0\n",
    "    structure_match_count = 0\n",
    "    \n",
    "    print(f\"Evaluating on {len(test_samples)} samples...\")\n",
    "    \n",
    "    for i, sample in enumerate(tqdm(test_samples)):\n",
    "        instruction = sample[\"instruction\"]\n",
    "        expected_response = sample[\"response\"]\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"Instruction: {instruction} \\nResponse:\\n\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs.input_ids.to(model.device),\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.3,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the generated response part\n",
    "        response_part = generated_text.split(\"Response:\")[-1].strip()\n",
    "        \n",
    "        # Parse the generated response\n",
    "        generated_json = parse_json_response(response_part)\n",
    "        expected_json = expected_response\n",
    "        \n",
    "        # Calculate metrics\n",
    "        exact_match = compare_exact_match(generated_json, expected_json)\n",
    "        structure_match = compare_structure_match(generated_json, expected_json)\n",
    "        key_overlap = calculate_key_overlap(generated_json, expected_json)\n",
    "        \n",
    "        if exact_match:\n",
    "            exact_match_count += 1\n",
    "        if structure_match:\n",
    "            structure_match_count += 1\n",
    "        \n",
    "        results.append({\n",
    "            \"instruction\": instruction,\n",
    "            \"expected\": expected_json,\n",
    "            \"generated\": generated_json,\n",
    "            \"exact_match\": exact_match,\n",
    "            \"structure_match\": structure_match,\n",
    "            \"key_overlap\": key_overlap,\n",
    "            \"raw_generated\": response_part\n",
    "        })\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    exact_match_accuracy = exact_match_count / len(test_samples)\n",
    "    structure_match_accuracy = structure_match_count / len(test_samples)\n",
    "    avg_key_overlap = sum(r[\"key_overlap\"] for r in results) / len(test_samples)\n",
    "    \n",
    "    # Print evaluation results\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Exact Match Accuracy: {exact_match_accuracy:.4f}\")\n",
    "    print(f\"Structure Match Accuracy: {structure_match_accuracy:.4f}\")\n",
    "    print(f\"Average Key Overlap: {avg_key_overlap:.4f}\")\n",
    "    print(f\"Samples evaluated: {len(test_samples)}\")\n",
    "    \n",
    "    # Save detailed results\n",
    "    output_results = {\n",
    "        \"metrics\": {\n",
    "            \"exact_match_accuracy\": exact_match_accuracy,\n",
    "            \"structure_match_accuracy\": structure_match_accuracy,\n",
    "            \"average_key_overlap\": avg_key_overlap,\n",
    "            \"total_samples\": len(test_samples)\n",
    "        },\n",
    "        \"detailed_results\": results\n",
    "    }\n",
    "    \n",
    "    with open(\"evaluation_results.json\", \"w\") as f:\n",
    "        json.dump(output_results, f, indent=2)\n",
    "    \n",
    "    # Print some examples\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"SAMPLE PREDICTIONS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for i in range(min(3, len(results))):\n",
    "        result = results[i]\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Instruction: {result['instruction']}\")\n",
    "        print(f\"Expected: {result['expected']}\")\n",
    "        print(f\"Generated: {result['generated']}\")\n",
    "        print(f\"Exact Match: {result['exact_match']}\")\n",
    "        print(f\"Structure Match: {result['structure_match']}\")\n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    return output_results\n",
    "\n",
    "def parse_json_response(text):\n",
    "    \"\"\"\n",
    "    Parse JSON from generated text, handling potential formatting issues\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try to find JSON pattern in the text\n",
    "        json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            json_str = json_match.group()\n",
    "            return json.loads(json_str)\n",
    "        else:\n",
    "            # If no JSON found, try to parse the entire text\n",
    "            return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Return empty dict if parsing fails\n",
    "        return {}\n",
    "\n",
    "def compare_exact_match(generated, expected):\n",
    "    \"\"\"\n",
    "    Check if generated response exactly matches expected response\n",
    "    \"\"\"\n",
    "    return generated == expected\n",
    "\n",
    "def compare_structure_match(generated, expected):\n",
    "    \"\"\"\n",
    "    Check if generated response has the same structure as expected\n",
    "    \"\"\"\n",
    "    if not isinstance(generated, dict) or not isinstance(expected, dict):\n",
    "        return False\n",
    "    \n",
    "    # Check if all expected keys are present\n",
    "    expected_keys = set(expected.keys())\n",
    "    generated_keys = set(generated.keys())\n",
    "    \n",
    "    return expected_keys.issubset(generated_keys)\n",
    "\n",
    "def calculate_key_overlap(generated, expected):\n",
    "    \"\"\"\n",
    "    Calculate the overlap of keys between generated and expected responses\n",
    "    \"\"\"\n",
    "    if not isinstance(generated, dict) or not isinstance(expected, dict):\n",
    "        return 0.0\n",
    "    \n",
    "    expected_keys = set(expected.keys())\n",
    "    generated_keys = set(generated.keys())\n",
    "    \n",
    "    if not expected_keys:\n",
    "        return 0.0\n",
    "    \n",
    "    intersection = expected_keys.intersection(generated_keys)\n",
    "    return len(intersection) / len(expected_keys)\n",
    "\n",
    "def evaluate_with_different_temperatures(model_path, test_data_path, temperatures=[0.1, 0.3, 0.7]):\n",
    "    \"\"\"\n",
    "    Evaluate model performance with different temperature settings\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nEvaluating with temperature {temp}...\")\n",
    "        \n",
    "        # Load model and tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "        )\n",
    "        \n",
    "        # Load test data\n",
    "        with open(test_data_path, 'r') as f:\n",
    "            test_data = json.load(f)\n",
    "        \n",
    "        test_samples = test_data[:20]  # Use smaller subset for temperature testing\n",
    "        \n",
    "        exact_match_count = 0\n",
    "        structure_match_count = 0\n",
    "        \n",
    "        for sample in tqdm(test_samples):\n",
    "            instruction = sample[\"instruction\"]\n",
    "            expected_response = sample[\"response\"]\n",
    "            \n",
    "            prompt = f\"Instruction: {instruction} \\nResponse:\\n\"\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    inputs.input_ids.to(model.device),\n",
    "                    max_new_tokens=256,\n",
    "                    temperature=temp,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=tokenizer.eos_token_id,\n",
    "                    num_return_sequences=1\n",
    "                )\n",
    "            \n",
    "            generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            response_part = generated_text.split(\"Response:\")[-1].strip()\n",
    "            generated_json = parse_json_response(response_part)\n",
    "            \n",
    "            if compare_exact_match(generated_json, expected_response):\n",
    "                exact_match_count += 1\n",
    "            if compare_structure_match(generated_json, expected_response):\n",
    "                structure_match_count += 1\n",
    "        \n",
    "        exact_match_acc = exact_match_count / len(test_samples)\n",
    "        structure_match_acc = structure_match_count / len(test_samples)\n",
    "        \n",
    "        results[temp] = {\n",
    "            \"exact_match_accuracy\": exact_match_acc,\n",
    "            \"structure_match_accuracy\": structure_match_acc\n",
    "        }\n",
    "        \n",
    "        print(f\"Temperature {temp}: Exact Match = {exact_match_acc:.4f}, Structure Match = {structure_match_acc:.4f}\")\n",
    "    \n",
    "    # Save temperature results\n",
    "    with open(\"temperature_evaluation.json\", \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "696322fa-88ed-4d62-ae8d-3b480ee73922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your model and test data\n",
    "model_path = \"./tinyllama-finetuned-detailed\" \n",
    "test_data_path = r\"C:\\Users\\T14 gen2\\Documents\\SementicSearch\\test_data.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f136c4f-8bfa-493c-8d2f-b0a2aac78bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data...\n",
      "Evaluating on 100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "100%|██████████| 100/100 [50:31<00:00, 30.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Exact Match Accuracy: 0.3500\n",
      "Structure Match Accuracy: 0.4500\n",
      "Average Key Overlap: 0.4500\n",
      "Samples evaluated: 100\n",
      "\n",
      "==================================================\n",
      "SAMPLE PREDICTIONS\n",
      "==================================================\n",
      "\n",
      "Sample 1:\n",
      "Instruction: Show the top 5 employees where salary greater than 37\n",
      "Expected: {'entity': 'employees', 'conditions': [{'field': 'salary', 'operator': '>', 'value': 37}], 'limit': 5, 'order': 'desc', 'order_by': 'salary'}\n",
      "Generated: {'entity': 'employees', 'conditions': [{'field': 'salary', 'operator': '>', 'value': 37}], 'limit': 5, 'order': 'desc', 'order_by': 'salary'}\n",
      "Exact Match: True\n",
      "Structure Match: True\n",
      "----------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "Instruction: Show the  customers where city is Mumbai and total_spent greater than 19 and age below 54\n",
      "Expected: {'entity': 'customers', 'conditions': [{'field': 'city', 'operator': '=', 'value': 'Mumbai'}, {'field': 'total_spent', 'operator': '>', 'value': 19}, {'field': 'age', 'operator': '<', 'value': 54}]}\n",
      "Generated: {'entity': 'customers', 'conditions': [{'field': 'city', 'operator': '=', 'value': 'Mumbai'}, {'field': 'total_spent', 'operator': '>', 'value': 19}, {'field': 'age', 'operator': '<', 'value': 54}]}\n",
      "Exact Match: True\n",
      "Structure Match: True\n",
      "----------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "Instruction: Show the minimum products where price greater than 47 and category is electronics\n",
      "Expected: {'entity': 'products', 'conditions': [{'field': 'price', 'operator': '>', 'value': 47}, {'field': 'category', 'operator': '=', 'value': 'electronics'}]}\n",
      "Generated: {'entity': 'products', 'conditions': [{'field': 'price', 'operator': '>', 'value': 47}, {'field': 'category', 'operator': '=', 'value': 'electronics'}]}\n",
      "Exact Match: True\n",
      "Structure Match: True\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_results = evaluate_finetuned_model(\n",
    "        model_path=model_path,\n",
    "        test_data_path=test_data_path,\n",
    "        num_samples=100  # Adjust based on your needs\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de69c22e-2b61-4685-8ee0-6d28a2d7e908",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
